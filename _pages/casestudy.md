---
permalink: /casestudy/
title: "Case Study: Algorithmic Short-Form Video and Youth Wellbeing"
author_profile: true
---

A focus on YouTube and TikTok and the effects on children.

![kid with phone](kid-with-phone.jpg)

## Abstract
Short-form video platforms like YouTube and TikTok play a big role in how kids and teenagers learn, connect with others, and spend their free time online. These platforms use recommendation algorithms that are designed to keep users watching for as long as possible through features like autoplay, endless scrolling, notifications, and fast-paced videos. While these tools can be fun and can help people learn or express creativity, they also raise ethical concerns about attention span, mental health, data collection, and how much responsibility tech companies have toward younger users.

This case study looks at the ethical issues surrounding algorithm-driven media and young people. It presents the issue from the perspectives of different audiences and encourages readers to think more deeply through interactive activities and discussion questions. Overall, the case argues that these concerns are not just about what is legal or about individual self-control, but also about how platforms are designed and the power companies have over user behavior.

## Background and Context
Over the past decade, social media has changed a lot, moving from mostly text-based posts to short, highly visual videos that are chosen by algorithms. Platforms like TikTok made very short videos popular, and YouTube later added Shorts alongside its longer videos. Today, these platforms are some of the most commonly used apps by children and teenagers.

Recommendation algorithms work by tracking things like how long users watch a video, what they like or comment on, and what they replay or share. Using this information, the algorithm tries to show content that will keep users watching for as long as possible. While this can already be hard for adults to manage, it may be even more difficult for children and teens, since their self-control and decision-making skills are still developing.

The main ethical issue is that the same design choices that help platforms grow and make money can also negatively affect young users’ wellbeing. Companies often describe heavy social media use as a personal choice or something parents should manage, but many critics argue that this ignores how strongly these systems are designed to influence user behavior.

## The Ethical Issue
The core ethical issue is whether it is acceptable for technology companies to deploy highly persuasive, engagement-optimized systems to young users who cannot fully understand or consent to their influence. These systems are legal in many jurisdictions, yet legality does not guarantee ethical responsibility.

Children and teens may experience:
- Reduced attention span and difficulty focusing
- Disrupted sleep patterns due to late-night scrolling
- Increased social comparison and anxiety
- Exposure to inappropriate or misleading content

At the same time, platforms benefit from increased engagement, advertising revenue, and data collection. This creates an uneven distribution of benefits and harms, raising questions of justice, care, and responsibility.

## Key Facts and Uncertainties
**Established facts**
- Recommendation systems on platforms like YouTube are explicitly designed to maximize engagement metrics such as watch time and interaction (Covington, Adams, and Sargin).

- Short-form and autoplay-based video formats increase session length and repeated consumption, particularly among younger users (Covington, Adams, and Sargin).

- Multiple studies link high levels of social media and mobile media use in children and adolescents with anxiety, depressive symptoms, and psychological distress (Keles et al.; Montag and Elhai).

- Research in pediatrics shows that frequent mobile device use among children is associated with challenges in attention, emotional regulation, and parent–child interaction (Radesky et al. 2015; Radesky et al. 2020).

- Adolescents experience both positive and negative effects from digital technology use, suggesting that harms are influenced by intensity, context, and design rather than technology alone (Orben and Przybylski).

**Uncertainties and debates**

- Long-term neurological and developmental effects of constant short-form media exposure remain under active study, particularly for very young children (Radesky et al. 2015).

- Parents report tension between the educational potential of mobile devices and concerns about overstimulation and dependency (Radesky et al. 2016).

- It remains contested how much responsibility should fall on families versus platform designers and regulators when harms emerge.

- Because these uncertainties involve vulnerable populations, ethical decision-making should prioritize precaution and care rather than waiting for complete scientific certainty.

All cited sources are listed [HERE.](citations/)

## Stakeholder Analysis
**Primary stakeholders**
- Children and adolescents: Most affected and least empowered to resist persuasive design
- Parents and guardians: Responsible for children’s wellbeing but limited in oversight and control

**Secondary stakeholders**
- Content creators: Depend on algorithms for visibility and income
- Technology companies: Design systems and profit from engagement

**Broader stakeholders**
- Educators managing attention and learning outcomes
- Policymakers tasked with protecting minors
- Society, which depends on the long-term wellbeing of future generations

Children’s interests arguably deserve greater ethical weight due to their developmental vulnerability.

## Ethical Analysis Using Multiple Lenses
**Duty / Deontological Ethics**
*What is my duty? How can human dignity be respected?*

From a duty-based perspective, children have basic rights that should be respected, including the right to privacy, healthy development, and protection from exploitation. Engagement-based recommendation systems can violate these rights because children are often unable to fully understand or consent to persuasive design techniques that are based on their data (Radesky et al. 2015).

Since children do not have the same level of autonomy as adults, using their attention mainly as a way to increase profit treats them as a means rather than as individuals who deserve respect. Even if these systems increase engagement or revenue, they are still unethical if they fail to respect children’s dignity and wellbeing.

**Contractarianism**
*Is this system fair for everyone involved?*

Looking at this issue through a justice lens, the benefits and harms of engagement-driven algorithms are not shared equally. Technology companies gain financial benefits, while children, families, and schools are left to deal with the negative effects, such as attention problems or emotional stress (Keles et al.).

From a Rawlsian point of view, this system is unfair. If people did not know whether they would be a child, a parent, or a tech executive, they likely would not agree to a system where children carry most of the risks. This shows that the current structure does not meet standards of fairness or justice.

**Utilitarian Ethics**
*How to achieve the greatest good?*

Utilitarianism focuses on overall outcomes. Short-form video platforms clearly provide benefits, such as entertainment, creativity, and social connection for many users. However, research suggests that excessive use can be linked to negative mental health outcomes for children and adolescents, including anxiety and attention issues (Montag and Elhai).

If these harms affect a large number of young users, the overall negative impact may outweigh the benefits. From a utilitarian perspective, reducing harm to children, even if it lowers engagement, could lead to better outcomes for society as a whole.

**Natural Law Ethics**
*What is naturally good for us?*

The common good approach focuses on conditions that allow people and communities to thrive. Healthy cognitive and emotional development in children is important not only for individuals, but also for education, civic participation, and long-term social stability.

Research showing connections between heavy media use and developmental challenges suggests that current algorithmic designs may work against these shared goals (Radesky et al. 2020). When systems prioritize engagement above all else, they may undermine what is naturally good for children and for society.

**Virtue Ethics**
*What would a virtuous person do?*

Virtue ethics emphasizes character rather than rules or consequences. From this perspective, ethical technology design requires qualities like responsibility, honesty, and self-restraint. Designing systems that intentionally exploit children’s attention weaknesses reflects poor character rather than ethical integrity.

A virtuous designer would aim to support users’ wellbeing and development, especially when those users are children. This may mean limiting certain features or redesigning systems in ways that prioritize long-term wellbeing over short-term profit.

**Care Ethics**
*How can we meet the needs of particular others?*

Care ethics is especially relevant in this case because children are dependent users. They rely on adults, institutions, and systems to protect their wellbeing. Ethical responsibility therefore does not fall only on children or parents, but also on the companies that design these platforms.

From this perspective, platforms should be designed with empathy for children’s developmental needs and family dynamics, rather than assuming that children have full self-control (Radesky et al. 2016). Ignoring these realities represents a failure to care for vulnerable users.

**Overall Takeaway**

Although these ethical frameworks approach the problem in different ways, they all point to the same conclusion: children’s vulnerability creates a higher ethical responsibility for technology designers. When algorithms are designed mainly to maximize engagement without considering developmental limits, they raise serious ethical concerns across multiple moral perspectives.

## Activity: You’re in Charge of the App!
**What's Going On?**

Imagine you get to help design a video app for kids and teens like you. You don’t have to code anything, you just get to make choices about how the app works. There are no “right” answers. The goal is to think about how design choices can affect how people feel and act.

[Click here to go to interactive activity!](activity/)

# Audience-Specific Perspectives

## For Kids and Teens: Why These Apps Are So Hard to Put Down
If you’ve ever opened TikTok or YouTube “just for a minute” and lost track of time, that’s not a personal failure. These apps are designed to keep your attention by quickly showing videos your brain finds exciting or familiar.

Watching videos can be fun and even educational, but nonstop scrolling can make it harder to focus, sleep, or feel good about yourself. Understanding how these systems work gives you more control.

Things you can try
- Turn off autoplay or notifications
- Set app timers or take scrolling breaks
- Notice how certain content affects your mood

## For Parents and Educators: Supporting Healthy Digital Habits
Short-form video platforms actively shape what children see and how long they stay engaged. While parental controls exist, many are optional and place the burden on families rather than designers.

Ethical concerns arise when platforms profit from systems that parents must constantly manage. Supporting children requires not only limits, but also conversations about how algorithms influence behavior and emotions.

## For Policymakers and Designers: Responsibility Beyond Choice
Relying solely on individual responsibility ignores the power of large-scale persuasive systems. Ethical governance may require default protections for minors, limits on data collection, and transparency about recommendation practices.

Balancing innovation with protection is challenging, but ethical responsibility increases when users are vulnerable.

## Discussion Questions
1. Should technology companies have greater ethical responsibility toward children than adults?

2. Where should responsibility lie between families, platforms, and governments?

3. Can engagement-based algorithms be ethically redesigned?

4. How might this issue differ across cultures or socioeconomic groups?

5. What would success look like for a youth-centered platform?

## Conclusion and Reflection
This case study shows us that ethical problems in computing don’t always come from bad intentions, but often from how systems are designed and what they are rewarded to do. When platforms are built mainly to make money and are used by kids or teens, the impact can go beyond what the law covers. This makes me think that responsibility doesn’t just fall on programmers, but also on designers, parents, users, and policymakers. Ethical computing isn’t about getting rid of technology, but about using and shaping it in a way that actually supports people’s wellbeing.